import torch  # å¯¼å…¥ PyTorch ä¸»åŒ…
import torch.nn as nn  # ä» PyTorch å¯¼å…¥ç¥ç»ç½‘ç»œæ¨¡å—
from transformers import (  # ä» transformers åº“ä¸­å¯¼å…¥æ‰€éœ€ç±»
    AutoModelForCausalLM,  # è‡ªåŠ¨åŠ è½½ç”¨äºå› æœè¯­è¨€å»ºæ¨¡çš„é¢„è®­ç»ƒæ¨¡å‹
    AutoTokenizer,  # è‡ªåŠ¨åŠ è½½å¯¹åº”çš„åˆ†è¯å™¨
    AutoConfig,  # è‡ªåŠ¨åŠ è½½æ¨¡å‹é…ç½®
    LogitsProcessorList, LogitsProcessor  # ç”Ÿæˆæ—¶ç”¨äºä¿®æ”¹ logits çš„å¤„ç†å™¨åŸºç±»åŠåˆ—è¡¨å®¹å™¨
)
import random  # Python å†…ç½®éšæœºæ•°åº“
from peft import get_peft_model, LoraConfig, TaskType  # PEFTï¼šåŠ è½½ LoRA é…ç½®å¹¶åŒ…è£…æ¨¡å‹ä»¥æ”¯æŒå‚æ•°é«˜æ•ˆå¾®è°ƒ

import torch  # å†æ¬¡å¯¼å…¥ torchï¼ˆé‡å¤å¯¼å…¥ä¸ä¼šå‡ºé”™ä½†å¯ä»¥çœç•¥ï¼‰
import torch.nn as nn  # å†æ¬¡å¯¼å…¥ nnï¼ˆåŒä¸Šï¼‰
import torch.nn.functional as F  # å¯¼å…¥å‡½æ•°å¼æ¥å£ï¼Œä¸€èˆ¬ç”¨äºæŸå¤±å‡½æ•°ç­‰


class FocalLoss(nn.Module):  # å®šä¹‰ Focal Loss æŸå¤±å‡½æ•°ç±»ï¼Œç»§æ‰¿ nn.Module
    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean', ignore_index=-100):  # åˆå§‹åŒ–è¶…å‚æ•°
        super(FocalLoss, self).__init__()  # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        self.alpha = alpha  # ç±»åˆ«ä¸å‡è¡¡æ—¶çš„ç¼©æ”¾ç³»æ•°
        self.gamma = gamma  # Focal Loss ä¸­çš„èšç„¦å› å­ Î³
        self.reduction = reduction  # æŸå¤±èšåˆæ–¹å¼ï¼š'mean'ã€'sum' æˆ– 'none'
        self.ignore_index = ignore_index  # æŒ‡å®šå¿½ç•¥çš„æ ‡ç­¾å€¼ï¼ˆç”¨äº paddingï¼‰

    def forward(self, inputs, targets, position_weights=None):  # å‰å‘è®¡ç®—æ¥å£
        """
        inputs: (B, L, C) or (B, C) logits
        targets: (B, L) or (B,) with class indices, may include -100 for ignore
        position_weights: (B, L) or (B,) or None
        """
        if inputs.dim() == 3:  # è‹¥è¾“å…¥ä¸ºåºåˆ—å½¢å¼ (B, L, C)
            B, L, C = inputs.shape  # è§£æ batchã€åºåˆ—é•¿åº¦å’Œç±»åˆ«æ•°
            inputs = inputs.reshape(B * L, C)  # å±•å¹³æˆ (B*L, C)ï¼Œæ–¹ä¾¿è®¡ç®—äº¤å‰ç†µ
            targets = targets.reshape(B * L)  # æ ‡ç­¾åŒæ ·å±•å¹³
            if position_weights is not None:  # è‹¥æœ‰ä½ç½®æƒé‡
                position_weights = position_weights.reshape(B * L)  # åŒæ ·å±•å¹³
        else:  # è‹¥è¾“å…¥ä¸ºæ™®é€šåˆ†ç±» (B, C)
            B, C = inputs.shape  # è§£æ batch å’Œç±»åˆ«æ•°
            targets = targets.reshape(B)  # æ ‡ç­¾å±•å¹³ä¸º (B,)
            if position_weights is not None:  # è‹¥æœ‰ä½ç½®æƒé‡
                position_weights = position_weights.reshape(B)  # å±•å¹³

        valid_mask = (targets != self.ignore_index).float()  # è®¡ç®—æœ‰æ•ˆæ ·æœ¬æ©ç ï¼Œå¿½ç•¥ ignore_index æ ‡ç­¾
        # Compute cross entropy (no reduction)
        ce_loss = F.cross_entropy(inputs, targets, reduction='none', ignore_index=self.ignore_index)  # (N,) é€æ ·æœ¬äº¤å‰ç†µæŸå¤±ï¼Œä¸åšèšåˆ
        pt = torch.exp(-ce_loss)  # p_t = exp(-CE)ï¼Œå³çœŸå®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss  # (N,) è®¡ç®— Focal Lossï¼šÎ±(1-p)^Î³ * CE

        # Apply optional weights
        if position_weights is not None:  # å¦‚æœæä¾›äº†ä½ç½®æƒé‡
            focal_loss = focal_loss * position_weights  # å¯¹æ¯ä¸ªä½ç½®ä¹˜ä»¥å¯¹åº”æƒé‡

        # Reduction
        if self.reduction == 'mean':  # è‹¥æŒ‡å®šåšå¹³å‡
            denom = (valid_mask * (position_weights if position_weights is not None else 1.0)).sum()  # æœ‰æ•ˆæ ·æœ¬åŠ æƒè®¡æ•°ä½œä¸ºåˆ†æ¯
            return focal_loss.sum() / (denom + 1e-8)  # é˜²æ­¢é™¤é›¶
        elif self.reduction == 'sum':  # è‹¥æŒ‡å®šæ±‚å’Œ
            return focal_loss.sum()  # è¿”å›æ€»æŸå¤±
        else:  # 'none'
            if inputs.dim() == 2:  # å¯¹äº (B, C) æƒ…å†µ
                return focal_loss.view(B)  # æ¢å¤åˆ° (B,)
            else:  # å¯¹äºåºåˆ— (B, L, C) æƒ…å†µ
                return focal_loss.view(B, L)  # æ¢å¤åˆ° (B, L)


class TsTokenFormatController(LogitsProcessor):  # è‡ªå®šä¹‰ logits å¤„ç†å™¨ï¼Œç”¨äºå¼ºçº¦æŸæ—¶åº token çš„ç”Ÿæˆæ ¼å¼
    def __init__(self, ts_token_range, ts_start_token_id, ts_end_token_id, ts_start_pos, ts_len):  # åˆå§‹åŒ–çº¦æŸå‚æ•°
        self.ts_token_start, self.ts_token_end = ts_token_range  # æ—¶åº token çš„ id èŒƒå›´ [start, end)
        self.ts_start_token_id = ts_start_token_id  # <TS_START> çš„ token id
        self.ts_end_token_id = ts_end_token_id  # <TS_END> çš„ token id
        self.ts_start_pos = ts_start_pos  # æ—¶åºç‰‡æ®µåœ¨ç”Ÿæˆåºåˆ—ä¸­çš„èµ·å§‹ä½ç½®ï¼ˆåŒ…å« <TS_START>ï¼‰
        self.ts_end_pos = ts_start_pos + 1 + ts_len  # æ—¶åºç‰‡æ®µç»“æŸä½ç½®ï¼ˆ<TS_START> + ts_len ä¸ªæ—¶åº token ä¹‹åï¼Œä½ç½®ç­‰äºå†™ <TS_END> çš„ä½ç½®ï¼‰

    def __call__(self, input_ids, scores):  # æ¯ä¸€æ­¥ç”Ÿæˆæ—¶ä¼šè¢«è°ƒç”¨ï¼Œä¿®æ”¹ scores åè¿”å›
        cur_len = input_ids.shape[1]  # å½“å‰å·²ç”Ÿæˆçš„ token é•¿åº¦ï¼ˆä¸å«æœ¬æ­¥å¾…é‡‡æ · tokenï¼‰
       
        mask = torch.full_like(scores, float("-inf"))  # åˆå§‹åŒ–ä¸€ä¸ªå…¨ä¸º -inf çš„ maskï¼Œç”¨æ¥ç¦æ­¢ä¸å…è®¸çš„ token

        if cur_len == self.ts_start_pos:  # å½“åˆ°è¾¾æ—¶åºç‰‡æ®µçš„èµ·å§‹ä½ç½®æ—¶
            mask[:, self.ts_start_token_id] = scores[:, self.ts_start_token_id]  # ä»…å…è®¸ç”Ÿæˆ <TS_START> è¿™ä¸ª token
            return mask  # è¿”å›è¢«çº¦æŸåçš„ logits

        elif self.ts_start_pos < cur_len < self.ts_end_pos:  # åœ¨ <TS_START> å’Œ <TS_END> ä¹‹é—´çš„ä½ç½®ï¼Œç”Ÿæˆçš„æ˜¯æ—¶åº token
            mask[:, self.ts_token_start:self.ts_token_end] = scores[:, self.ts_token_start:self.ts_token_end]  # ä»…å…è®¸åœ¨æ—¶åº token èŒƒå›´å†…é‡‡æ ·

            # ä¸‹é¢è¿™è¡Œ topk åªæ˜¯ä¾‹å­/è°ƒè¯•ï¼Œç»“æœæ²¡æœ‰è¢«ä½¿ç”¨
            topk = torch.topk(mask[:, self.ts_token_start:self.ts_token_end], k=5, dim=-1)  # å–å‰ 5 ä¸ªæœ€å¤§ logitsï¼ˆæœªå®é™…ç”¨åˆ°ï¼‰

            return mask  # è¿”å›çº¦æŸåçš„ logits

        elif cur_len == self.ts_end_pos:  # å½“åˆ°è¾¾æ—¶åºç‰‡æ®µç»“æŸä½ç½®æ—¶
            mask[:, self.ts_end_token_id] = scores[:, self.ts_end_token_id]  # ä»…å…è®¸ç”Ÿæˆ <TS_END> token
            return mask  # è¿”å›çº¦æŸåçš„ logits
        else:
            return scores  # å…¶ä»–ä½ç½®ä¸åšä»»ä½•çº¦æŸï¼Œè¿”å›åŸå§‹ logits


class Model(nn.Module):  # ä¸»æ¨¡å‹ç±»ï¼ŒåŒ…è£… Qwen è¯­è¨€æ¨¡å‹å¹¶æ‰©å±•æ—¶åº token èƒ½åŠ›
    def __init__(self, configs):  # åˆå§‹åŒ–ï¼Œconfigs ä¸ºè‡ªå®šä¹‰é…ç½®å¯¹è±¡
        super(Model, self).__init__()  # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        self.configs = configs  # ä¿å­˜é…ç½®
        config = AutoConfig.from_pretrained(self.configs.local_model_path)  # ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹é…ç½®
        self.d_model = config.hidden_size  # æ¨¡å‹éšè—å±‚ç»´åº¦
        self.text_tokenizer = AutoTokenizer.from_pretrained(self.configs.local_model_path)  # ä»æœ¬åœ°è·¯å¾„åŠ è½½åˆ†è¯å™¨
        self.text_tokenizer.pad_token = self.text_tokenizer.eos_token  # å°† pad_token è®¾ç½®ä¸º eos_tokenï¼Œé¿å… pad ä¸ºç©º

        # æ·»åŠ æ—¶åºç‰¹æ®Štoken
        special_tokens_dict = {  # å®šä¹‰éœ€è¦æ·»åŠ çš„é¢å¤–ç‰¹æ®Š token
        'additional_special_tokens': ['<TS_START>', '<TS_END>']  # ç”¨äºæ ‡è®°æ—¶åºç‰‡æ®µçš„å¼€å§‹å’Œç»“æŸ
        }

        self.text_tokenizer.add_special_tokens(special_tokens_dict)  # å‘ tokenizer ä¸­æ³¨å†Œè¿™äº›ç‰¹æ®Š token
        # self.text_tokenizer.apply_chat_template  # é¢„ç•™ï¼šå¯é€‰æ‹©åº”ç”¨ chat æ¨¡æ¿ï¼ˆå½“å‰æœªä½¿ç”¨ï¼‰
        
        self.n_embed = self.configs.elected_n_embed  # ä¸ºæ—¶åº token é¢„ç•™çš„ embedding æ•°é‡
        # åˆå§‹åŒ–Qwenæ¨¡å‹
        self.model = self._initialize_model(config)  # æ ¹æ®é…ç½®åˆå§‹åŒ– Qwen æ¨¡å‹
        # åˆå§‹åŒ–åµŒå…¥å±‚
        self._initialize_embedding_layer()  # æ‰©å±• embedding ä»¥å®¹çº³æ—¶åº token å’Œç‰¹æ®Š token

        self._initialize_output_layer(config)  # æ„å»ºå¹¶æ›¿æ¢è¾“å‡ºå±‚ï¼Œä½¿å…¶ä¸æ‰©å±•åçš„ embedding å¯¹é½

        if self.configs.layers:  # è‹¥æŒ‡å®šåªè®­ç»ƒéƒ¨åˆ†å±‚ï¼ˆåˆ†å±‚å¾®è°ƒï¼‰
            num_layers = len(self.model.model.layers)  # è·å– Transformer æ€»å±‚æ•°
            print(f"Qwen2.5 å…±æœ‰ {num_layers} å±‚ Transformer")  # æ‰“å°å±‚æ•°ä¿¡æ¯
            for param in self.model.model.parameters():  # å…ˆå°† backbone æ‰€æœ‰å‚æ•°å†»ç»“
                param.requires_grad = False
            n_unfreeze = self.configs.n_layers  # ä»é…ç½®ä¸­è¯»å–éœ€è¦è§£å†»çš„å±‚æ•°ï¼ˆé€šå¸¸ä¸ºæœ«å°¾è‹¥å¹²å±‚ï¼‰
            print(n_unfreeze)  # æ‰“å°è§£å†»å±‚æ•°

            for i in range(num_layers - n_unfreeze, num_layers):  # è§£å†»æœ€å n_unfreeze å±‚ Transformer
                for param in self.model.model.layers[i].parameters():
                    param.requires_grad = True  # å…è®¸è¿™äº›å±‚å‚ä¸è®­ç»ƒ
            
            # âœ… Step 3: è§£å†» embedding å±‚
            for param in self.model.model.embed_tokens.parameters():  # è§£å†»è¯åµŒå…¥å±‚
                param.requires_grad = True

            # âœ… Step 4: è§£å†»è¾“å‡ºå±‚ï¼ˆlm_headï¼‰
            for param in self.model.lm_head.parameters():  # è§£å†»è¯­è¨€æ¨¡å‹å¤´éƒ¨ï¼ˆè¾“å‡ºå±‚ï¼‰
                param.requires_grad = True

        if self.configs.frozen:  # å¦‚æœé…ç½®è¦æ±‚æ•´ä½“å†»ç»“æ¨¡å‹
            # å…¨éƒ¨å†»ç»“
            for param in self.parameters():  # å…ˆå†»ç»“å½“å‰æ¨¡å‹æ‰€æœ‰å‚æ•°
                param.requires_grad = False

            # è§£å†»åµŒå…¥å±‚
            for param in self.model.model.embed_tokens.parameters():  # ä»…è§£å†» embedding å±‚
                param.requires_grad = True

            # è§£å†»è¾“å‡ºå±‚
            for param in self.model.lm_head.parameters():  # ä»…è§£å†»è¾“å‡ºå±‚
                param.requires_grad = True

        # å¦‚æœå¯ç”¨ LoRA
        if self.configs.use_lora:
            print("ğŸ”§ Applying LoRA to model...")  # æ‰“å°æç¤ºä¿¡æ¯
            lora_config = LoraConfig(  # æ„é€  LoRA é…ç½®
                r=8,  # LoRA rank
                lora_alpha=32,  # LoRA ç¼©æ”¾å› å­
                lora_dropout=0.1,  # LoRA dropout æ¦‚ç‡
                bias="none",  # ä¸å¯¹ bias ä½¿ç”¨ LoRA
                task_type=TaskType.CAUSAL_LM,  # å› ä¸ºæ˜¯è‡ªå›å½’è¯­è¨€æ¨¡å‹ä»»åŠ¡
                target_modules=["q_proj", "v_proj"]  # æŒ‡å®šåº”ç”¨ LoRA çš„å­æ¨¡å—åç§°ï¼ˆQ/K/V ä¸­çš„ Q/Vï¼‰
            )
            self.model = get_peft_model(self.model, lora_config)  # å°†åŸå§‹æ¨¡å‹åŒ…è£…ä¸º LoRA æ¨¡å‹
            

    def _initialize_model(self, config):  # å†…éƒ¨æ–¹æ³•ï¼šæ ¹æ®é…ç½®åˆå§‹åŒ–æ¨¡å‹
        if self.configs.params:  # å¦‚æœéœ€è¦ä»é¢„è®­ç»ƒæƒé‡åŠ è½½
            return AutoModelForCausalLM.from_pretrained(
                self.configs.local_model_path,  # æœ¬åœ°æ¨¡å‹è·¯å¾„
                output_attentions=True,  # åœ¨å‰å‘ä¸­è¾“å‡ºæ³¨æ„åŠ›
                output_hidden_states=True,  # åœ¨å‰å‘ä¸­è¾“å‡ºå„å±‚éšè—çŠ¶æ€
                trust_remote_code=True  # ä¿¡ä»»è¿œç¨‹è‡ªå®šä¹‰æ¨¡å‹ä»£ç 
            )
        else:  # å¦åˆ™ä»…æ ¹æ® config ä»å¤´åˆå§‹åŒ–æ¨¡å‹å‚æ•°
            return AutoModelForCausalLM.from_config(config, trust_remote_code=True)

    def _initialize_embedding_layer(self, use_normal_dist=True):  # å†…éƒ¨æ–¹æ³•ï¼šæ‰©å±•å¹¶åˆå§‹åŒ– embedding å±‚
        original_weight = self.model.model.embed_tokens.weight  # åŸå§‹è¯åµŒå…¥æƒé‡çŸ©é˜µ (V, d)
        self.original_len = len(original_weight)  # åŸå§‹è¯è¡¨å¤§å° V

        # ğŸ”¸ è·å– special token æ•°é‡
        special_tokens_len = len(self.text_tokenizer.additional_special_tokens)  # é¢å¤–ç‰¹æ®Š token çš„ä¸ªæ•°

        if use_normal_dist:  # è‹¥é‡‡ç”¨é«˜æ–¯åˆ†å¸ƒè¿›è¡Œåˆå§‹åŒ–
            mu = torch.mean(original_weight, dim=0)  # è®¡ç®—åŸ embedding çš„å‡å€¼å‘é‡ Î¼
            n = original_weight.size()[0]  # è¯è¡¨å¤§å° n
            sigma = ((original_weight - mu).T @ (original_weight - mu)) / n  # ç®€å•ä¼°è®¡åæ–¹å·®çŸ©é˜µ Î£
            dist = torch.distributions.multivariate_normal.MultivariateNormal(mu, covariance_matrix=1e-5*sigma)  # æ„é€ å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼ˆåæ–¹å·®ç¼©å° 1e-5ï¼‰

            ts_weight = torch.stack([dist.sample() for _ in range(self.n_embed)], dim=0)  # ä¸ºæ—¶åº token é‡‡æ · n_embed ä¸ª embedding å‘é‡
            special_tokens_weight = torch.stack([dist.sample() for _ in range(special_tokens_len)], dim=0)  # ä¸º special tokens é‡‡æ · embedding å‘é‡
        else:  # è‹¥é‡‡ç”¨ä»åŸ embedding ä¸­éšæœºé‡‡æ ·çš„æ–¹å¼åˆå§‹åŒ–
            random.seed(self.configs.seed)  # å›ºå®šéšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
            sample_indices = random.sample(range(len(original_weight)), self.n_embed)  # éšæœºé€‰æ‹© n_embed ä¸ªç´¢å¼•
            ts_weight = original_weight[sample_indices]  # é€‰å–å¯¹åº”çš„ embedding ä½œä¸ºæ—¶åº token çš„æƒé‡

            special_indices = random.sample(range(len(original_weight)), special_tokens_len)  # ä¸º special tokens éšæœºé€‰æ‹©è‹¥å¹²ç´¢å¼•
            special_tokens_weight = original_weight[special_indices]  # ç›´æ¥å¤åˆ¶å¯¹åº” embedding

        # ğŸ”¸ æ‰©å±•è¯è¡¨
        total_vocab_size = self.original_len + self.n_embed + special_tokens_len  # æ‰©å®¹åçš„è¯è¡¨æ€»å¤§å°
        self.model.resize_token_embeddings(total_vocab_size)  # è°ƒæ•´æ¨¡å‹ embedding å’Œè¾“å‡ºå±‚çš„è¯è¡¨å¤§å°

        # ğŸ”¸ èµ‹å€¼æ–°åµŒå…¥
        start_idx = self.original_len  # æ—¶åº token çš„èµ·å§‹ç´¢å¼•
        end_idx = start_idx + self.n_embed  # æ—¶åº token çš„ç»“æŸç´¢å¼•
        self.model.model.embed_tokens.weight.data[start_idx:end_idx] = ts_weight  # å°†æ—¶åº token çš„æƒé‡å†™å…¥ embedding

        start_idx = end_idx  # special token çš„èµ·å§‹ç´¢å¼•
        end_idx = start_idx + special_tokens_len  # special token çš„ç»“æŸç´¢å¼•
        self.model.model.embed_tokens.weight.data[start_idx:end_idx] = special_tokens_weight  # å°† special token çš„æƒé‡å†™å…¥ embedding

        # ğŸ”¸ ä¿å­˜ embedding æƒé‡ä»¥ä¾›è¾“å‡ºå±‚ä½¿ç”¨
        self.embedding_weight = self.model.model.embed_tokens.weight  # ä¿å­˜å…±äº«çš„ embedding æƒé‡å¼•ç”¨


    def _initialize_output_layer(self, config):  # å†…éƒ¨æ–¹æ³•ï¼šåˆ›å»ºå¹¶æ›¿æ¢è¾“å‡ºå±‚
        # åˆ›å»ºè¾“å‡ºå±‚ï¼Œä¸embedding layerå…±äº«æƒé‡
        output_layer = nn.Linear(config.hidden_size, self.embedding_weight.size(0), bias=False)  # çº¿æ€§å±‚è¾“å‡ºç»´åº¦ç­‰äºè¯è¡¨å¤§å°
        # ä½¿ç”¨embedding layerçš„æƒé‡åˆå§‹åŒ–è¾“å‡ºå±‚
        output_layer.weight.data = self.embedding_weight.data  # ç›´æ¥å…±äº« embedding æƒé‡
        
        # æ›¿æ¢Qwenæ¨¡å‹çš„è¾“å‡ºå±‚
        self.model.set_output_embeddings(output_layer)  # å°†çº¿æ€§å±‚æ³¨å†Œä¸ºæ¨¡å‹çš„è¾“å‡º embeddings
        self.model.lm_head.weight = self.model.model.embed_tokens.weight  # ç¡®ä¿ lm_head ä¸ embed_tokens æƒé‡ç»‘å®šï¼ˆæƒé‡å…±äº«ï¼‰
        
    def forward(self, inputs):     # å‰å‘ä¼ æ’­æ¥å£ï¼Œæ¥æ”¶è‡ªå®šä¹‰ inputs å­—å…¸
        text_ids, input_ids, labels = inputs['text_ids'], inputs['ts_ids'], inputs['labels']  # è§£æè¾“å…¥ä¸­çš„æ–‡æœ¬ idã€æ—¶åº id ä»¥åŠæ ‡ç­¾
        device = input_ids.device  # è·å–å½“å‰å¼ é‡æ‰€åœ¨è®¾å¤‡

        # æ„é€  attention mask
        attention_mask = torch.ones(input_ids.shape[0], input_ids.shape[1], dtype=torch.float32, device=device)  # ç®€å•åœ°ä¸ºé padding è¾“å…¥æ„é€ å…¨ 1 çš„æ³¨æ„åŠ› maskï¼ˆæ­¤å¤„æœªè€ƒè™‘ paddingï¼‰

        # æ„é€ æ–° token çš„ä½ç½®æƒé‡ï¼ˆä¾‹å¦‚åŠ å¤§æ–°å¼•å…¥ token çš„æŸå¤±æƒé‡ï¼‰
        new_token_weight = self.configs.new_token_weight if hasattr(self.configs, 'new_token_weight') else 1  # è‹¥é…ç½®ä¸­å­˜åœ¨ new_token_weightï¼Œåˆ™ä½¿ç”¨å®ƒï¼Œå¦åˆ™é»˜è®¤ä¸º 1
        orig_token_weight = 1  # åŸå§‹è¯è¡¨ token çš„æƒé‡ä¸º 1
        position_weights = torch.where(labels >= self.original_len, new_token_weight, orig_token_weight)  # å¦‚æœ label çš„ id è¶…è¿‡åŸè¯è¡¨é•¿åº¦ï¼Œè®¤ä¸ºæ˜¯æ–° tokenï¼Œä½¿ç”¨ new_token_weight
        ts_start_id = self.text_tokenizer.convert_tokens_to_ids("<TS_START>")  # è·å– <TS_START> çš„ idï¼ˆå½“å‰æœªç›´æ¥ä½¿ç”¨ï¼‰
        ts_end_id = self.text_tokenizer.convert_tokens_to_ids("<TS_END>")  # è·å– <TS_END> çš„ idï¼ˆå½“å‰æœªç›´æ¥ä½¿ç”¨ï¼‰

        # ğŸš€ æ­£ç¡®åœ°ä¼  input_idsï¼Œåˆ«ç”¨ inputs_embedsï¼
        outputs = self.model(  # è°ƒç”¨åº•å±‚ Qwen æ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­
            input_ids=input_ids,  # è¾“å…¥ token id åºåˆ—
            labels=labels,  # è¯­è¨€æ¨¡å‹è®­ç»ƒæ ‡ç­¾åºåˆ—ï¼ˆshifted å†…éƒ¨å¤„ç†ï¼‰
            attention_mask=attention_mask,  # æ³¨æ„åŠ› mask
            output_hidden_states=True  # è¾“å‡ºéšè—çŠ¶æ€ï¼ˆä¾¿äºè°ƒè¯•æˆ–åç»­ä½¿ç”¨ï¼‰
        )

        loss_fn = FocalLoss(alpha=1.0, gamma=2.0, reduction='mean')  # æ„é€  FocalLoss æŸå¤±å‡½æ•°
        loss = loss_fn(outputs.logits[..., :-1, :], labels[..., 1:], position_weights[..., 1:])  # ä½¿ç”¨ teacher-forcing æ–¹å¼æ‰‹åŠ¨å¯¹é½ logits ä¸æ ‡ç­¾ï¼ˆå³ç§»ä¸€ä¸ªä½ç½®ï¼‰
        outputs.loss = loss  # å°†è‡ªå®šä¹‰çš„ loss å†™å› outputs å¯¹è±¡ï¼Œæ–¹ä¾¿å¤–éƒ¨ç»Ÿä¸€è®¿é—®
        
        return outputs  # è¿”å›åŒ…å« lossã€logits ç­‰ä¿¡æ¯çš„è¾“å‡ºå¯¹è±¡


    def gen_ts(self, inputs, text_token_len=112, ts_token_len=12):  # æ ¹æ®è¾“å…¥ç”Ÿæˆæ—¶åº token åºåˆ—
        tokenizer = self.text_tokenizer  # ä½¿ç”¨å·²æ‰©å±•çš„ tokenizer
        device = next(self.model.parameters()).device  # è·å–æ¨¡å‹æ‰€åœ¨è®¾å¤‡
        original_len = self.original_len  # åŸå§‹è¯è¡¨é•¿åº¦
        n_ts_token = self.n_embed  # æ—¶åº token ä¸ªæ•°
        ts_token_range = (original_len, original_len + n_ts_token)  # æ—¶åº token åœ¨æ–°è¯è¡¨ä¸­çš„ id èŒƒå›´

        input_ids = inputs['ts_ids']  # è·å–è¾“å…¥åºåˆ—ï¼ˆè¿™é‡Œ ts_ids å®é™…ä¸ŠåŒ…å«äº†æ–‡æœ¬ + æ—¶åºä½ç½®æ¨¡æ¿ï¼‰

        device = next(self.model.parameters()).device  # å†æ¬¡ç¡®å®šè®¾å¤‡ï¼ˆå¯çœç•¥ï¼‰
        attention_mask = (input_ids != tokenizer.pad_token_id).long().to(device)  # æ ¹æ® pad_token_id æ„é€  0/1 æ³¨æ„åŠ› mask
        input_ids = input_ids.to(device)  # å°† input_ids ç§»åˆ°æ¨¡å‹è®¾å¤‡

        ts_end_token_id = tokenizer.convert_tokens_to_ids("<TS_END>")  # è·å– <TS_END> id
        ts_start_token_id = tokenizer.convert_tokens_to_ids("<TS_START>")  # è·å– <TS_START> id
        max_len = text_token_len + ts_token_len + 2  # æœ€å¤§æ–°ç”Ÿæˆ token æ•°ï¼ˆæ–‡æœ¬ + TS_START/END + ts_token_len çš„ä¸Šç•Œï¼‰

        logits_processor = LogitsProcessorList([  # æ„å»º logits å¤„ç†å™¨åˆ—è¡¨
            TsTokenFormatController(
                ts_token_range=ts_token_range,     # å‡è®¾æ—¶åºtoken idæ˜¯è¿™ä¸ªèŒƒå›´
                ts_start_token_id=ts_start_token_id,          # <TS_START>
                ts_end_token_id=ts_end_token_id,            # <TS_END>
                ts_start_pos=text_token_len + input_ids.shape[1],                  # æ–‡æœ¬ token ä¸ºå‰ text_token_len ä¸ªï¼›è¿™é‡Œçš„å†™æ³•éœ€ç»“åˆå…·ä½“æ‹¼æ¥æ–¹å¼ç†è§£
                ts_len=ts_token_len                     # æ—¶åº token è¾“å‡ºé•¿åº¦
            )
        ])

        generated = self.model.generate(  # ä½¿ç”¨ huggingface generate æ¥å£è¿›è¡Œè‡ªå›å½’ç”Ÿæˆ
            input_ids=input_ids,  # åˆå§‹è¾“å…¥åºåˆ—ï¼ˆpromptï¼‰
            attention_mask=attention_mask,  # å¯¹åº”çš„ attention mask
            max_new_tokens=max_len,  # é™åˆ¶ç”Ÿæˆçš„æ–° token æ•°é‡
            eos_token_id=tokenizer.convert_tokens_to_ids("<|im_end|>"),  # æŒ‡å®šç»ˆæ­¢ token id
            pad_token_id=tokenizer.pad_token_id,  # æŒ‡å®š padding token id
            return_dict_in_generate=True,  # è¿”å›å­—å…¸å½¢å¼ç»“æœï¼ŒåŒ…å« sequencesã€scores ç­‰
            logits_processor=logits_processor  # åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ³¨å…¥è‡ªå®šä¹‰ logits çº¦æŸ
        )

        return generated.sequences[:, input_ids.shape[1]:]  # åªè¿”å›æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰åŸå§‹ prompt éƒ¨åˆ†ï¼‰


    @staticmethod
    def init_weights_kaiming(m):  # é™æ€æ–¹æ³•ï¼šå¯¹çº¿æ€§å±‚ä½¿ç”¨ Kaiming åˆå§‹åŒ–
        if isinstance(m, nn.Linear):  # åˆ¤æ–­æ¨¡å—ç±»å‹æ˜¯å¦ä¸º Linear
            nn.init.kaiming_normal_(m.weight, nonlinearity="leaky_relu")  # ä½¿ç”¨ Kaiming æ­£æ€åˆå§‹åŒ–æƒé‡ï¼Œé€‚é… leaky_relu
            m.bias.data.fill_(0.01)  # å°† bias åˆå§‹åŒ–ä¸ºå¸¸æ•° 0.01
